{"bgColorIndex":0,"textColorIndex":0,"note":"## Overview\nThis repository covers a methodology for combining and processing multiple datasets. It covers three stages:\n1. Data ingestion into Amazon S3.\n2. Transformation of source datasets into a target format using AWS Glue.\n3. Storage back to Amazon S3 of the combined data sets.\n\n## Architecture\n![Alt text](\/architecture\/DataProcessor.png)\n\n## AWS Glue ETL Flow\n![Alt text](\/architecture\/GlueETL_Flow.png)\n\n## Tools and Technologies\n- Amazon S3\n- Amazon EventBridge\n- AWS Lambda\n- AWS Glue\n\n## Prerequisites\n1. Ensure you have an active AWS Account\n2. Create a unique bucket and record the name. This name serves as an input parameter (GlueScriptsBucketName) in the CloudFormation template.\n3. Once the bucket is created, copy the file \"glue-etl-script.py\" into it. You can find this file in the \"\/scripts\" folder.\n3. Confirm the availability of the files \"merchants.csv\" and \"receiptdata.csv.\" These sample data files can be found in the \"\/source-data\" folder. You are welcome to modify the data content as needed, but please refrain from altering the header fields in the CSV files.\n\n## How to run AWS CloudFormation stack?\nThe \"data-processor.yaml\" file located in the \"\/cloud-formation\" folder contains the essential Infrastructure as Code (IAC) instructions to create the required AWS resources. Here is an overview:\n1. Creates the necessary IAM roles.\n2. Creates Lambda function.\n3. Creates AWS Glue job that loads the script from a S3 bucket, which was created as part of the prerequisites outlined earlier.\n4. Creates EventBridge rule.\n5. Creates the source and target buckets.\n\n### Update the bucket names in the Glue job script\n1. Access the Glue job service from the AWS management console.\n2. Click on the glue job (acpy-glue-job) that was created as part of the CloudFormation stack.\n3. Edit the â€˜Scriptsâ€™ tab, and replace the bucket names that were created as part of CloudFormation stack\n    s3:\/\/REPLACE-WITH-SOURCE-MERCHANT-BUCKET-NAME\" with SourceMerchantBucketName value.\n    s3:\/\/REPLACE-WITH-SOURCE-RAW-NAME\" with SourceRawBucketName value.\n    s3:\/\/REPLACE-WITH-TARGET-BUCKET-NAME\" with TargetBucketName value.\n4. Click â€˜Saveâ€™.\n\n## Validation\n1. The source data sets include two sample files - \"merchants.csv\" and \"receiptdata.csv,\", can be found in the \"\/source-data\" folder.\n2. Copy these files to the respective S3 buckets that are created as part of the CloudFormation run:\n    - Copy merchants.csv into 'SORUCE-MERCHANT-BUCKET'\n    - Copy receiptdata.csv into 'SORUCE-RAW-BUCKET'\n3. Upon successful upload of \"receiptdata.csv\" to the 'SORUCE-RAW-BUCKET' bucket, AWS EventBridge rule will be triggered automatically. This rule will invoke the Lambda function, leading to the execution of the associated Glue job, responsible for transforming the CSV file into its target state.\n4. Please feel free to examine the 'run details' of the AWS Glue job. The transformation may require up to 2 minutes to be fully executed.\n5. Refer to the 'TARGET-BUCKET' bucket to access the output file.\n6. Repeat the process by updating either the \"merchants.csv\" or \"receiptdata.csv\" files as required.\n\n## Cleanup\n1. After completing the validations, make sure to empty the S3 buckets that were created as part of the CloudFormation stack. This step helps in maintaining data cleanliness and avoiding any unintentional data retention.\n2. If necessary, delete the CloudFormation stack to release AWS resources."}